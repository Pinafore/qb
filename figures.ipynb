{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from functional import seq, pseq\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from os import path, makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import (\n",
    "    ggplot, aes, facet_grid, facet_wrap,\n",
    "    geom_histogram, geom_density, geom_segment, geom_text, geom_bar, geom_violin, geom_boxplot, geom_step, geom_vline,\n",
    "    xlab, ylab, ggtitle, geom_line,\n",
    "    scale_color_manual, scale_fill_manual, scale_fill_discrete, scale_y_continuous, scale_color_discrete,\n",
    "    coord_flip, theme,\n",
    "    stat_ecdf, stat_ydensity, element_text, scale_x_continuous, geom_dotplot,\n",
    "    scale_x_log10, scale_y_log10\n",
    ")\n",
    "COLORS = [\n",
    "    '#49afcd', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
    "    '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'\n",
    "]\n",
    "\n",
    "output_path = 'output/plots/'\n",
    "makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_question(q):\n",
    "    q['n_sentences'] = len(q['tokenizations'])\n",
    "    if q['subcategory'] == 'None':\n",
    "        q['subcategory'] = None\n",
    "    q['sentences'] = [q['text'][start:end] for start, end in q['tokenizations']]\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/external/datasets/qanta.mapped.2018.04.18.json') as f:\n",
    "    questions = [format_question(q) for q in json.load(f)['questions'] if 'test' not in q['fold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/external/wikidata-claims_instance-of.jsonl') as f:\n",
    "    claims = [json.loads(l) for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = defaultdict(set)\n",
    "country_synonyms = {\n",
    "    'former country',\n",
    "    'member state of the United Nations',\n",
    "    'sovereign state',\n",
    "    'member state of the European Union',\n",
    "    'permanent member of the United Nations Security Council',\n",
    "    'member state of the Council of Europe'\n",
    "}\n",
    "\n",
    "for r in claims:\n",
    "    title = r['title']\n",
    "    if title is None:\n",
    "        continue\n",
    "    else:\n",
    "        title = title.replace(' ', '_')\n",
    "        obj = r['object']\n",
    "        if r['property'] == 'instance of' and obj is not None:\n",
    "            if 'Wikimedia' in obj:\n",
    "                continue\n",
    "            if obj in country_synonyms:\n",
    "                properties[title].add('country')\n",
    "            else:\n",
    "                properties[title].add(obj)\n",
    "\n",
    "object_properties = Counter()\n",
    "for r in claims:\n",
    "    if r['title'] is not None and r['object'] is not None and 'Wikimedia' not in r['object']:\n",
    "        object_properties[r['object']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_properties.most_common(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_question_property_counts(answers):\n",
    "    question_property_counts = Counter()\n",
    "    for page in answers:\n",
    "        if page in properties:\n",
    "            q_props = properties[page]\n",
    "            if 'human' in q_props:\n",
    "                if len(q_props) == 1:\n",
    "                    question_property_counts['human'] += 1\n",
    "                    continue\n",
    "                elif 'human biblical character' in q_props:\n",
    "                    question_property_counts['human biblical character'] += 1\n",
    "                    continue\n",
    "                elif 'Catholic saint' in q_props:\n",
    "                    question_property_counts['Catholic saint'] += 1\n",
    "            for prop in q_props:\n",
    "                question_property_counts[prop] += 1\n",
    "    return question_property_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qb_question_property_counts = compute_question_property_counts([q['page'] for q in questions])\n",
    "qb_question_property_counts.most_common(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_answer_type_assignments(question_property_counts, answers):\n",
    "    answer_type_assignments = {}\n",
    "    for page in answers:\n",
    "        if page in properties:\n",
    "            q_props = list(properties[page])\n",
    "            if len(q_props) == 1:\n",
    "                answer_type_assignments[page] = q_props[0]\n",
    "            else:\n",
    "                prop_counts = [question_property_counts[prop] for prop in q_props]\n",
    "                props_with_counts = zip(prop_counts, q_props)\n",
    "                _, most_common_prop = max(props_with_counts)\n",
    "                answer_type_assignments[page] = most_common_prop\n",
    "        else:\n",
    "            answer_type_assignments[page] = 'NOMATCH'\n",
    "    return answer_type_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qb_answer_type_assignments = compute_answer_type_assignments(qb_question_property_counts, [q['page'] for q in questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(questions)):\n",
    "    q = questions[i]\n",
    "    q['instance of'] = qb_answer_type_assignments[q['page']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(questions)\n",
    "\n",
    "def trash_to_pc(cat):\n",
    "    if cat == 'Trash':\n",
    "        return 'Pop Culture'\n",
    "    else:\n",
    "        return cat\n",
    "df.category = df.category.map(trash_to_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.n_sentences.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.page.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_len = df.n_sentences.mean()\n",
    "median_len = df.n_sentences.median()\n",
    "stats_df = pd.DataFrame([\n",
    "    {'n': mean_len, ' ': 'Mean # of Sentences'},\n",
    "    {'n': median_len, ' ': 'Median # of Sentences'}\n",
    "])\n",
    "p = (\n",
    "    ggplot(df)\n",
    "    + aes(x='n_sentences')\n",
    "    + geom_histogram(binwidth=1)\n",
    "    + geom_segment(\n",
    "        aes(x='n', xend='n', y=0, yend=29000, color=' '),\n",
    "        stats_df\n",
    "    )\n",
    "    + xlab('Number of Sentences in Question')\n",
    "    + ylab('Count')\n",
    "    + scale_color_manual(values=COLORS)\n",
    ")\n",
    "p.save(path.join(output_path, 'n_sentence_histogram.pdf'))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop anything without a sub-category\n",
    "plot_df = df.dropna(subset=['subcategory'])\n",
    "plot_df = plot_df[plot_df.subcategory != '']\n",
    "top_categories = {'Science', 'Literature', 'History', 'Fine Arts'}\n",
    "plot_df = plot_df[plot_df.category.isin(top_categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = plot_df.groupby(['category', 'subcategory']).count().reset_index()\n",
    "grouped_df['count'] = grouped_df['answer'] # rename a counts column\n",
    "grouped_df = grouped_df[['category', 'subcategory', 'count']]\n",
    "grouped_df = grouped_df[grouped_df['count'] > 10]\n",
    "grouped_df.sort_values(['category', 'count'], inplace=True)\n",
    "grouped_df.reset_index(inplace=True, drop=True)\n",
    "grouped_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a lebling function is hacky, but I couldn't get list of strings working\n",
    "lookup = dict(zip(grouped_df['index'], grouped_df['subcategory']))\n",
    "def make_label(arr):\n",
    "    labels = []\n",
    "    for x in arr:\n",
    "        if x in lookup:\n",
    "            labels.append(lookup[x])\n",
    "        else:\n",
    "            labels.append(x)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (\n",
    "    ggplot(grouped_df)\n",
    "    + facet_wrap('category', scales='free')\n",
    "    + aes(x='index', y='count', fill='category')\n",
    "    + geom_bar(stat='identity')\n",
    "    + xlab('Sub-Category') + ylab('Count') + coord_flip()\n",
    "    + theme(panel_spacing_x=1.4, panel_spacing_y=.3, figure_size=(8, 5))\n",
    "    + scale_fill_discrete(name=\"Category\")\n",
    "    + scale_x_continuous(breaks=grouped_df['index'], labels=make_label)\n",
    ")\n",
    "p.save(path.join(output_path, 'subcategories.pdf'))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "category_df = df.copy()\n",
    "category_list = category_df['category'].value_counts().index.tolist()\n",
    "category_list.reverse()\n",
    "category_cat = CategoricalDtype(categories=category_list, ordered=True)\n",
    "category_df['category_cat'] = category_df['category'].astype(str).astype(category_cat)\n",
    "\n",
    "p = (\n",
    "    ggplot(category_df)\n",
    "    + aes(x='category_cat', fill='category_cat')\n",
    "    + geom_bar(show_legend=False)\n",
    "    + xlab('Category') + ylab('Count') + coord_flip()\n",
    "    + theme(figure_size=(5, 6))\n",
    ")\n",
    "p.save(path.join(output_path, 'categories.pdf'))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/external/squad/train-v1.1.json') as f:\n",
    "    squad_dataset = json.load(f)['data']\n",
    "squad_questions = []\n",
    "squad_titles = set()\n",
    "for page in squad_dataset:\n",
    "    squad_titles.add(page['title'])\n",
    "    for cqa in page['paragraphs']:\n",
    "        for qa in cqa['qas']:\n",
    "            squad_questions.append(qa['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(squad_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_percentile = .95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQuAD Question Lengths\n",
    "squad_word_dist = pseq(squad_questions).map(lambda s: len(nltk.word_tokenize(s))).list()\n",
    "squad_word_dist = np.array(squad_word_dist)\n",
    "squad_word_df = pd.DataFrame({'n': squad_word_dist})\n",
    "squad_word_df['source'] = '# Words in Question'\n",
    "squad_word_df['dataset'] = 'SQuAD'\n",
    "squad_quantile = squad_word_df['n'].quantile(outlier_percentile)\n",
    "squad_word_df = squad_word_df[squad_word_df.n <= squad_quantile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TriviaQA Question Lengths\n",
    "with open('data/external/triviaqa/unfiltered-web-train.json') as f:\n",
    "    tqa_data = json.load(f)['Data']\n",
    "tqa_questions = []\n",
    "tqa_answers = []\n",
    "for q in tqa_data:\n",
    "    if q['Answer']['Type'] == 'WikipediaEntity':\n",
    "        tqa_questions.append(q['Question'])\n",
    "        tqa_answers.append(q['Answer']['MatchedWikiEntityName'].replace(' ', '_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(tqa_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqa_word_dist = pseq(tqa_questions).map(lambda s: len(nltk.word_tokenize(s))).list()\n",
    "tqa_word_dist = np.array(tqa_word_dist)\n",
    "tqa_word_df = pd.DataFrame({'n': tqa_word_dist})\n",
    "tqa_word_df['source'] = '# Words in Question'\n",
    "tqa_word_df['dataset'] = 'TriviaQA'\n",
    "tqa_quantile = tqa_word_df['n'].quantile(outlier_percentile)\n",
    "tqa_word_df = tqa_word_df[tqa_word_df.n <= tqa_quantile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqa_question_property_counts = compute_question_property_counts(tqa_answers)\n",
    "tqa_question_property_counts.most_common(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqa_answer_type_assignments = compute_answer_type_assignments(tqa_question_property_counts, tqa_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleQuestions QuestionLengths\n",
    "with open('data/external/simplequestions/annotated_fb_data_train.txt') as f:\n",
    "    sq_questions = []\n",
    "    sq_answers = []\n",
    "    for line in f:\n",
    "        splits = line.split('\\t')\n",
    "        sq_questions.append(splits[3].strip())\n",
    "        sq_answers.append(splits[2].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_word_dist = pseq(sq_questions).map(lambda s: len(nltk.word_tokenize(s))).list()\n",
    "sq_word_dist = np.array(sq_word_dist)\n",
    "sq_word_df = pd.DataFrame({'n': sq_word_dist})\n",
    "sq_word_df['source'] = '# Words in Question'\n",
    "sq_word_df['dataset'] = 'SimpleQuestions'\n",
    "sq_quantile = sq_word_df['n'].quantile(outlier_percentile)\n",
    "sq_word_df = sq_word_df[sq_word_df.n <= sq_quantile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jeopardy\n",
    "with open('data/external/jeopardy/jeopardy_questions.json') as f:\n",
    "    j_questions = []\n",
    "    j_answers = []\n",
    "    j_data = json.load(f)\n",
    "    for q in j_data:\n",
    "        j_questions.append(q['question'])\n",
    "        j_answers.append(q['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_word_dist = pseq(j_questions).map(lambda s: len(nltk.word_tokenize(s))).list()\n",
    "j_word_dist = np.array(j_word_dist)\n",
    "j_word_df = pd.DataFrame({'n': j_word_dist})\n",
    "j_word_df['source'] = '# Words in Question'\n",
    "j_word_df['dataset'] = 'Jeopardy!'\n",
    "j_quantile = j_word_df['n'].quantile(outlier_percentile)\n",
    "j_word_df = j_word_df[j_word_df.n <= j_quantile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiz Bowl Question Lengths\n",
    "sent_word_dist = pseq(questions).flat_map(lambda q: q['sentences']).map(lambda s: len(nltk.word_tokenize(s))).list()\n",
    "question_word_dist = pseq(questions).map(lambda q: q['text']).map(lambda s: len(nltk.word_tokenize(s))).list()\n",
    "sent_word_dist = np.array(sent_word_dist)\n",
    "question_word_dist = np.array(question_word_dist)\n",
    "sent_word_df = pd.DataFrame({'n': sent_word_dist})\n",
    "sent_word_df['source'] = '# Words in Question'\n",
    "sent_word_df['dataset'] = 'QB (sentence)'\n",
    "sent_quantile = sent_word_df['n'].quantile(outlier_percentile)\n",
    "sent_word_df = sent_word_df[sent_word_df.n <= sent_quantile]\n",
    "\n",
    "question_word_df = pd.DataFrame({'n': question_word_dist})\n",
    "question_word_df['source'] = '# Words in Question'\n",
    "question_word_df['dataset'] = 'QB (question)'\n",
    "question_quantile = question_word_df['n'].quantile(outlier_percentile)\n",
    "question_word_df = question_word_df[question_word_df.n <= question_quantile]\n",
    "\n",
    "quiz_bowl_word_df = pd.concat([sent_word_df, question_word_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.concat([quiz_bowl_word_df, squad_word_df, tqa_word_df, sq_word_df, j_word_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (\n",
    "    ggplot(word_df) + aes(x='dataset', y='n')\n",
    "    + geom_violin(aes(fill='dataset', color='dataset'), trim=True, show_legend={'color': False})# + geom_boxplot(outlier_shape=None, outlier_alpha=0, width=.1)\n",
    "    + xlab('Dataset') + ylab('Distribution of Length in Words') + coord_flip()\n",
    "    + scale_fill_discrete(name='Dataset')\n",
    ")\n",
    "#p.save(path.join(output_path, 'length_dist.pdf'))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ggplot(word_df) + facet_wrap('dataset', nrow=1) + aes(x='n')\n",
    "    + geom_histogram(binwidth=2)\n",
    "    + xlab('Dataset') + ylab('Distribution of Length in Words') + coord_flip()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ggplot() + aes(x='n') + facet_wrap('source', scales='free')\n",
    "    + geom_histogram(data=sent_word_df, binwidth=2)\n",
    "    + geom_histogram(data=question_word_df, binwidth=2)\n",
    "    + theme(panel_spacing_x=.5, figure_size=(8, 3))\n",
    "    + xlab('Length of Example') + ylab('Count')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qb_type_df = df.groupby(['instance of', 'category']).count().reset_index()\n",
    "qb_type_df['n'] = qb_type_df['qanta_id'] #  This is a dummie column for counts\n",
    "qb_type_df['dataset'] = 'Quiz Bowl'\n",
    "#qb_type_df = qb_type_df[qb_type_df['instance of'] != 'NOMATCH']\n",
    "qb_type_df = qb_type_df.sort_values('n', ascending=False)[['instance of', 'category', 'n', 'dataset']][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqa_counts = Counter(tqa_answer_type_assignments.values())\n",
    "tqa_type_df = pd.DataFrame(tqa_counts.most_common(30), columns=['instance of', 'n'])\n",
    "tqa_type_df['category'] = 'NA'\n",
    "tqa_type_df['dataset'] = 'TriviaQA'\n",
    "#tqa_type_df = tqa_type_df[tqa_type_df['instance of'] != 'NOMATCH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type_df = pd.concat([qb_type_df, tqa_type_df])\n",
    "type_df = qb_type_df\n",
    "\n",
    "ordered_categories = list(type_df.groupby('instance of').sum().reset_index().sort_values('n', ascending=True)['instance of'])\n",
    "type_df['instance of'] = pd.Categorical(type_df['instance of'], categories=ordered_categories, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (\n",
    "    ggplot(type_df) + aes(x='instance of', y='n', fill='category')\n",
    "    + geom_bar(stat='identity') + coord_flip()\n",
    "    + xlab('Wikidata Type (\"instance of\")') + ylab('Count')\n",
    "    + scale_fill_discrete(name=\"Category\") + theme(figure_size=(4, 3))\n",
    ")\n",
    "p.save(path.join(output_path, 'ans_type_dist.pdf'))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (\n",
    "    ggplot(tqa_type_df[tqa_type_df['instance of'] != 'NOMATCH']) + aes(x='instance of', y='n')\n",
    "    + geom_bar(stat='identity') + coord_flip()\n",
    "    + xlab('Wikidata.org \"instance of\" Value') + ylab('Count')\n",
    ")\n",
    "p.save(path.join(output_path, 'ans_type_dist.pdf'))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qb_answers = list(df.page.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qb_a_counts = Counter([a for a in qb_answers if a is not None])\n",
    "j_a_counts = Counter(j_answers)\n",
    "tqa_a_counts = Counter(tqa_answers)\n",
    "sq_a_counts = Counter(sq_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_answer_count_df(counts, name):\n",
    "    rows = []\n",
    "    for n in counts.values():\n",
    "        rows.append({'n': n, 'dataset': name})\n",
    "    ac_df = pd.DataFrame(rows)\n",
    "    total = ac_df.n.sum()\n",
    "    ac_df['p'] = ac_df.n / total\n",
    "    ac_df = ac_df.sort_values('n', ascending=False)\n",
    "    ac_df['cdf'] = ac_df.p.cumsum()\n",
    "    ac_df['x'] = list(range(1, len(ac_df) + 1))\n",
    "    ac_df['r'] = ac_df['x'] / (len(ac_df) + 1)\n",
    "    ac_df['n_cumulative'] = ac_df.n.cumsum()\n",
    "    return ac_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_count_df = pd.concat([\n",
    "    create_answer_count_df(qb_a_counts, 'Quiz Bowl'),\n",
    "    create_answer_count_df(j_a_counts, 'Jeopardy!'),\n",
    "    create_answer_count_df(tqa_a_counts, 'TriviaQA'),\n",
    "    create_answer_count_df(sq_a_counts, 'SimpleQuestions')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_scale(breakpoints):\n",
    "    return [f'{100 * b:.0f}%' for b in breakpoints]\n",
    "\n",
    "p = (\n",
    "    ggplot(answer_count_df) + aes(x='r', y='n_cumulative', color='dataset')\n",
    "    + geom_step()\n",
    "    + xlab('Number of Unique Answers') + ylab('Percent Coverage')\n",
    "    #+ scale_y_continuous(labels=percent_scale, breaks=np.linspace(0, 1, num=11))\n",
    ")\n",
    "p.save(path.join(output_path, 'unique_answer_coverage.pdf'))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

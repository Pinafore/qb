{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from qanta.datasets.quiz_bowl import QuestionDatabase\n",
    "from qanta.util.constants import GUESSER_TRAIN_FOLD, GUESSER_DEV_FOLD\n",
    "from qanta.guesser.rnn_entity import RnnEntityGuesser, BatchedDataset,\\\n",
    "    clean_question, repackage_hidden\n",
    "from qanta.guesser.nn import convert_text_to_embeddings_indices\n",
    "from qanta.preprocess import tokenize_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "guesser = RnnEntityGuesser.load('output/guesser/qanta.guesser.rnn_entity.RnnEntityGuesser/')\n",
    "guesser.model = guesser.model.cuda()\n",
    "criterion = nn.CrossEntropyLoss(reduce=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RnnEntityModel(\n",
       "  (dropout): Dropout(p=0.25)\n",
       "  (word_embeddings): Embedding(139580, 300, padding_idx=0)\n",
       "  (rnn): WeightDrop(\n",
       "    (module): GRU(300, 1000, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "  )\n",
       "  (classification_layer): Sequential(\n",
       "    (0): Linear(in_features=2000, out_features=8297)\n",
       "    (1): BatchNorm1d(8297, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): Dropout(p=0.15)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_grads = {}\n",
    "\n",
    "def extract_grad_hook(name):\n",
    "    def hook(grad):\n",
    "        extracted_grads[name] = grad\n",
    "    return hook\n",
    "\n",
    "def extract_grad_hook(name):\n",
    "    def hook(grad):\n",
    "        extracted_grads[name] = grad\n",
    "    return hook\n",
    "\n",
    "guesser.model = guesser.model.cuda()\n",
    "guesser.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions = QuestionDatabase().all_questions().values()\n",
    "questions = [q for q in questions if q.fold == GUESSER_DEV_FOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stuff(question_list):\n",
    "    x_test_tokens = [x for x in question_list]\n",
    "    y_test = np.zeros(len(question_list))\n",
    "    dataset = BatchedDataset(\n",
    "        guesser.batch_size, guesser.multi_embedding_lookup, guesser.rel_position_vocab, guesser.rel_position_lookup,\n",
    "        x_test_tokens, y_test,\n",
    "        truncate=False, shuffle=False, train=False\n",
    "    )\n",
    "\n",
    "    grads = []\n",
    "    outputs = []\n",
    "    losses = []\n",
    "    hidden = guesser.model.init_hidden(guesser.batch_size)\n",
    "\n",
    "    for b in range(len(dataset.t_x_w_batches)):\n",
    "        t_x_w_batch = Variable(dataset.t_x_w_batches[b])\n",
    "        t_x_pos_batch = Variable(dataset.t_x_pos_batches[b])\n",
    "        t_x_iob_batch = Variable(dataset.t_x_iob_batches[b])\n",
    "        t_x_type_batch = Variable(dataset.t_x_type_batches[b])\n",
    "        t_x_mention_batch = Variable(dataset.t_x_mention_batches[b])\n",
    "\n",
    "        length_batch = dataset.length_batches[b]\n",
    "        sort_batch = dataset.sort_batches[b]\n",
    "\n",
    "        hidden = guesser.model.init_hidden(len(length_batch))\n",
    "        \n",
    "#         if len(length_batch) != guesser.batch_size:\n",
    "#             # This could happen for the last batch which is shorter than batch_size\n",
    "#             hidden = guesser.model.init_hidden(len(length_batch))\n",
    "#         else:\n",
    "#             hidden = repackage_hidden(hidden, reset=True)\n",
    "\n",
    "        guesser.model.eval()\n",
    "        out, hidden = guesser.model(\n",
    "            t_x_w_batch, t_x_pos_batch, t_x_iob_batch, t_x_type_batch, t_x_mention_batch,\n",
    "            length_batch, hidden, extract_grad_hook('embed')\n",
    "        )\n",
    "        scores, preds = torch.max(out, 1) # take gradient w.r.t top guess\n",
    "        outputs.append(out.data)\n",
    "\n",
    "        guesser.model.zero_grad()\n",
    "        loss = criterion(out, preds)\n",
    "        losses.append(loss.data)\n",
    "        \n",
    "        loss.sum().backward()\n",
    "        batch_size, length = t_x_w_batch.size()\n",
    "        embed_grad = extracted_grads['embed'].contiguous()\n",
    "        embed = guesser.model.word_embeddings(t_x_w_batch)\n",
    "        onehot_grad = embed.view(-1) * embed_grad.view(-1)\n",
    "        onehot_grad = onehot_grad.view(batch_size, length, -1).sum(-1)\n",
    "        onehot_grad = onehot_grad.data.cpu().numpy()\n",
    "        grads.append(onehot_grad)\n",
    "    \n",
    "    grads = np.concatenate(grads)\n",
    "    outputs = torch.cat(outputs)\n",
    "    losses = torch.cat(losses)\n",
    "    \n",
    "    return grads, outputs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def greedy_remove(question_list):\n",
    "#     _xs = [list(guesser.nlp(clean_question(x.flatten_text()))) for x in question_list]\n",
    "#     _ys = [x[0][0] for x in guesser.guess(_xs, 10, tokenize=False)]\n",
    "\n",
    "#     removed_indices = [[] for _ in question_list]\n",
    "#     indices = [list(range(len(x))) for x in _xs]\n",
    "#     alive = [True for _ in _xs]\n",
    "    \n",
    "#     xs = list(_xs)\n",
    "        \n",
    "#     while True:\n",
    "#         onehot_grad, out, loss = get_onehot_grad(xs)\n",
    "#         for i, x in enumerate(xs):\n",
    "#             if len(x) == 1:\n",
    "#                 alive[i] = False # stop removing when there is only one token left\n",
    "#             if alive[i]:\n",
    "#                 drop_idx = np.argmax(onehot_grad[i][:len(x)])\n",
    "#                 removed_indices[i].append(indices[i][drop_idx])\n",
    "#                 indices[i] = indices[i][:drop_idx] + indices[i][drop_idx + 1:]\n",
    "#                 x = x[:drop_idx] + x[drop_idx + 1:]\n",
    "#             xs[i] = x\n",
    "\n",
    "#         if sum(alive) == 0:\n",
    "#             break\n",
    "        \n",
    "#         pred = [x[0][0] for x in guesser.guess(xs, 10, tokenize=False)]\n",
    "#         for i, (x, y, z) in enumerate(zip(xs, _ys, pred)):\n",
    "#             if z != y:\n",
    "#                 alive[i] = False\n",
    "#                 removed_indices[i] = removed_indices[i][:-1]\n",
    "#     return removed_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_remove(question, max_beam_size=10):\n",
    "    original = guesser.guess([question], 10, tokenize=False)[0][0][0]\n",
    "    \n",
    "    removed_indices = [[]]\n",
    "    indices = [list(range(len(question)))]\n",
    "    \n",
    "    xs = [list(question)]\n",
    "    \n",
    "    while True:        \n",
    "        print(len(xs[0]), end=' ')\n",
    "        \n",
    "        if len(xs) == 0:\n",
    "            break\n",
    "        \n",
    "        assert len(removed_indices) == len(xs)\n",
    "        assert len(indices) == len(xs)\n",
    "        \n",
    "        onehot_grad, _, _ = get_stuff(xs)\n",
    "        new_xs = []\n",
    "        new_removed = []\n",
    "        new_indices = []\n",
    "\n",
    "        for i, x in enumerate(xs):\n",
    "            order = np.argsort(onehot_grad[i][:len(x)])[:max_beam_size]\n",
    "            for k in order:\n",
    "                new_xs.append(x[:k] + x[k + 1:])\n",
    "                new_removed.append(removed_indices[i] + [indices[i][k]])\n",
    "                new_indices.append(indices[i][:k] + indices[i][k + 1:])\n",
    "        \n",
    "        guesses = [x[0] for x in guesser.guess(new_xs, 10, tokenize=False)]\n",
    "        # print(sum(g == original for g, s in guesses))\n",
    "        \n",
    "        indices = [(i, (g, s)) for i, (g, s) in enumerate(guesses) if g == original]\n",
    "        indices = [i for i, _ in sorted(indices, key=lambda x: x[1][1])[:max_beam_size]]\n",
    "        if len(indices) == 0:\n",
    "            return removed_indices\n",
    "        \n",
    "        xs = [new_xs[i] for i in indices]\n",
    "        removed_indices = [new_removed[i] for i in indices]\n",
    "        indices = [new_indices[i] for i in indices]\n",
    "        # guesses = [x[0][0] for x in guesser.guess(xs, 10, tokenize=False)]\n",
    "        # print(sum(x == original for x in guesses))\n",
    "        # print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch0/shifeng/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py:325: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n",
      "/scratch0/shifeng/qb/qanta/guesser/rnn_entity.py:687: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 73 72 71 70 69 68 67 66 65 64 63 62 61 60 59 58 57 56 55 54 53 52 51 50 49 48 47 46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 \n",
      "53 52 51 50 49 48 47 46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 \n",
      "77 76 75 74 73 72 71 70 69 68 67 66 65 64 63 62 61 60 59 58 57 56 55 54 53 52 51 50 49 48 47 46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 \n",
      "74 73 72 71 70 69 68 67 66 65 64 63 62 61 60 59 58 57 56 55 54 53 52 51 50 49 48 47 46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 \n",
      "43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 \n"
     ]
    }
   ],
   "source": [
    "removed_indices = []\n",
    "for q in questions[:5]:\n",
    "    q = list(guesser.nlp(clean_question(q.flatten_text())))\n",
    "    removed_indices.append(beam_search_remove(q, 20)[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Around_the_World_in_Eighty_Days', 0.16779573)\n",
      "fix ('Around_the_World_in_Eighty_Days', 0.088940158)\n",
      "\n",
      "\n",
      "('Alexander_the_Great', 0.99782467)\n",
      "horned ('Alexander_the_Great', 0.02978701)\n",
      "\n",
      "\n",
      "('Seminole_Wars', 0.10996531)\n",
      "wars them costing mexican everglades its ('Seminole_Wars', 0.032432083)\n",
      "\n",
      "\n",
      "('Joseph_Stalin', 0.35837778)\n",
      "this soviet epidemics occidental ('Joseph_Stalin', 0.017363174)\n",
      "\n",
      "\n",
      "('Gulf_Stream', 0.37491512)\n",
      "water current ('Gulf_Stream', 0.062081344)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch0/shifeng/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py:325: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n",
      "/scratch0/shifeng/qb/qanta/guesser/rnn_entity.py:687: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(out)\n"
     ]
    }
   ],
   "source": [
    "for i, removed in enumerate(removed_indices):\n",
    "    q = list(guesser.nlp(clean_question(questions[i].flatten_text())))\n",
    "    guesses = guesser.guess([q], 10, tokenize=False)[0][0]\n",
    "    print(guesses)\n",
    "    qq = [w for i, w in enumerate(q) if i not in removed]\n",
    "    guesses = guesser.guess([qq], 10, tokenize=False)[0][0]\n",
    "    print(' '.join(w.lower_ for w in qq), guesses)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# question_list = questions[:30]\n",
    "# xs = [guesser.nlp(clean_question(x.flatten_text())) for x in question_list]\n",
    "# removed_indices = greedy_remove(question_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(30):\n",
    "#     print('Original Question')\n",
    "#     print(' '.join([x.lower_ for x in xs[i]]))\n",
    "#     print()\n",
    "#     x_afterfor i in range(30):\n",
    "#     print('Original Question')\n",
    "#     print(' '.join([x.lower_ for x in xs[i]]))\n",
    "#     print()\n",
    "#     x_after = [w for j, w in enumerate(xs[i]) if j not in removed_indices[i]]\n",
    "#     print('Modified Question')\n",
    "#     print(' '.join([x.lower_ for x in x_after]))\n",
    "#     print()\n",
    "#     print('Original Guesses')\n",
    "#     for g, s in guesser.guess([xs[i]], 4, tokenize=False)[0]:\n",
    "#         print(g, s)\n",
    "#     print()\n",
    "#     print('Modified Guesses')\n",
    "#     for g, s in guesser.guess([x_after], 4, tokenize=False)[0]:\n",
    "#         print(g, s)\n",
    "#     print()\n",
    "#     print() = [w for j, w in enumerate(xs[i]) if j not in removed_indices[i]]\n",
    "#     print('Modified Question')\n",
    "#     print(' '.join([x.lower_ for x in x_after]))\n",
    "#     print()\n",
    "#     print('Original Guesses')\n",
    "#     for g, s in guesser.guess([xs[i]], 4, tokenize=False)[0]:\n",
    "#         print(g, s)\n",
    "#     print()\n",
    "#     print('Modified Guesses')\n",
    "#     for g, s in guesser.guess([x_after], 4, tokenize=False)[0]:\n",
    "#         print(g, s)\n",
    "#     print()\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
